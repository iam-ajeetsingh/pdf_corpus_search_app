[
  {
    "chunk_id": "pdf3_0",
    "text": "ACTING EARLY",
    "source_file": "pdf3.txt"
  },
  {
    "chunk_id": "pdf3_1",
    "text": "WHEN THE\nWORLD ISN\u2019T",
    "source_file": "pdf3.txt"
  },
  {
    "chunk_id": "pdf3_2",
    "text": "WATCHING",
    "source_file": "pdf3.txt"
  },
  {
    "chunk_id": "pdf3_3",
    "text": "LESSONS FROM",
    "source_file": "pdf3.txt"
  },
  {
    "chunk_id": "pdf3_4",
    "text": "ANTICIPATORY",
    "source_file": "pdf3.txt"
  },
  {
    "chunk_id": "pdf3_5",
    "text": "ACTION IN",
    "source_file": "pdf3.txt"
  },
  {
    "chunk_id": "pdf3_6",
    "text": "ETHIOPIA IN 2021\n1\nCredits\nCover photo: On 10 May 2022 in the internally displaced person (IDP) site of Guyah, Afar Region, Ethiopia, 13-year-old Keria fetches water from a UNICEF\nsupported water point. Credit: UNICEF/Sewunet\nContent:\nKate Katch, kate.katch@un.org\nActing early when the world\nisn\u2019t watching\nLessons from anticipatory\naction in Ethiopia in 2021\nHighlights\n\u2022 For Ethiopia, living with climate volatility requires \u2022 Three out of five recipients of assistance report-\nanticipating droughts and floods and mitigating ed an improved ability to afford food, livelihood\ntheir impact on the most vulnerable people. inputs and household bills.\n\u2022 In 2020, OCHA facilitated an anticipatory ac- \u2022 In a world where humanitarian crises compete\ntion framework in Ethiopia to get ahead of for political attention and resources, Ethiopia\ndroughts and reduce their impact on vulnerable proves that anticipatory action frameworks make\ncommunities. acting early the default.\n\u2022 OCHA activated the pilot framework in December \u2022 The pilot taught valuable lessons in how to im-\n2020, while political and humanitarian focus was prove anticipatory action\u2019s impact in the context\non the Tigray conflict in Ethiopia. of protracted and overlapping crises.\n\u2022 The Central Emergency Response Fund released\nUS$20 million to reduce the drought\u2019s impact on\nmore than 900,000 people. The allocation was\none of the earliest responses to the 2020-2022\ndrought.\nSeeds ready for distribution at Chereti woreda,Tofik Kebele on 13 June 2021. Credit: FAO/Tewelde\n3\nSummary\nWhen a country is hit by a sudden disaster \u2013 an But this time there was a glimmer of hope. The\nearthquake, a tsunami or floods \u2013 the impact is humanitarian community acted earlier thanks to\nimmediate, the crisis makes headlines and aid the UN Office for the Coordination of Humanitari-\noften follows. But when a country is hit by a slow- an Affairs (OCHA) piloting an anticipatory action\nmoving disaster, such as a drought, it can take framework. In December 2020 \u2013 earlier than any\nmonths or even years to see the devastating impact. other donor \u2013 OCHA\u2019s Central Emergency Response\nThese crises unfold in slow motion: crops fail, Fund (CERF) provided US$20 million to humanitari-\nlivestock die, people sell their assets and leave their an partners to reduce the drought\u2019s impact on more\nhomes, women and girls risk their safety as they than 900,000 people.\nwalk further to find water, and children are pulled\nfrom school. According to a satisfaction survey caried out by\n60_decibels, more than 60 per cent of the recipients\nDroughts are predictable, but they too often fail to of assistance reported improvements in their\nreceive funding until suffering reaches catastrophic overall quality of life.1 The pilot framework also\nlevels and dramatic images hit the media. This has taught valuable lessons in what anticipatory action\nbeen Ethiopia\u2019s experience, time and time again. means in a prolonged and complex crisis. As\nJoyce Msuya, the Assistant Secretary-General for\nBetween October 2020 and June 2022, Ethiopia Humanitarian Affairs and Deputy Emergency Relief\nexperienced five consecutive below-average rainy Coordinator, concluded:\nseasons, with the 2022 March-May season being\nthe driest in 70 years. The drought\u2019s prolonged \u201cIn 2024, drought is once again driving millions of\nnature ultimately affected an estimated people to suffer. The Ethiopia anticipatory action\n17 million people. pilot demonstrated that if we act earlier, people\u2019s\nlives will be better. We have the evidence, now we\nmust scale.\u201d\n1 Ethiopia survey report by 60 Decibels.\nFirtun Yakob Omar, 21 a mother of 2(L) and her father Yakob Omar Menur, 52 a father of 18 (R) dropping seeds in their farmland supported by FAO in\nDolo-Ado woreda, Kelemisge kebele on 12 June 2021. Credit: FAO/Tewelde\n4\n18 July 2022. Eli Dar, Afar region. Eli Dar, Afar region. A girl, Fatima, walks with a water container. In eastern Afar, the community at the outskirts of the\ntown of Eli Dar have been experiencing the impact of drought. The grounds that would normally provide a year or more worth of water for the village\ndried up about 6 months ago and temperatures in the area are into the 40s. Credit: OCHA/Loh-Taylor\nChallenge\nEthiopia is highly vulnerable to severe seasonal is now more opportunity to limit the humanitarian\ndroughts; it has experienced them every decade impact of droughts. However, despite early warning\nsince 1953. They are compounded by conflict, information, the humanitarian community has re-\nlocust invasions, high food prices and localized peatedly struggled to act due to a lack of resources\nflooding, meaning communities in Ethiopia continu- and competing global priorities.\nously receive humanitarian aid.\nFollowing Ethiopia\u2019s 2015-2018 droughts \u2013 the\nOver CERF\u2019s 18-year history, Ethiopia has consist- worst in 30 years \u2013 an inter-agency humanitari-\nently ranked among its top five recipient countries, an evaluation found that the response to those\nhaving received $500 million in CERF funding. droughts saved lives, but it was not timely.2 An\nApproximately 40 per cent of that funding has gone earlier response could have stopped people from\nto drought relief. selling assets to purchase food and pay bills. It\ncould have avoided sharp increases in severe acute\nAs Ethiopia is one of the countries most vulner- malnutrition. Fewer children could have dropped\nable to climate change, climate volatility will get out of school and many illnesses could have\nworse over the coming years. Positively, advances been prevented.\nin weather-and-climate forecasting means there\n2 IASC, Inter-Agency Humanitarian Evaluation of the Drought Response in Ethiopia (February 2020).\n5\nSolution\nIn 2020, OCHA decided to pilot an anticipatory ac- (by June 2021). However, not all seasonal climate\ntion framework in Ethiopia to see how the human- forecasts met the threshold \u2013 some were more\nitarian community could get ahead of drought and certain in predicting a below-average season than\nreduce its impact on vulnerable communities. others.\nAnticipatory action frameworks use a model or fore- The forecasts\u2019 uncertainty did not stop a no-regrets\ncast to trigger pre-arranged funding for pre-agreed approach. To manage the risk of providing funding\nactivities. In essence, a framework pre-determines and a drought not occurring, CERF decided for the\nexactly when funding will be released and who will first time to split the allocation in two stages. In\ntake what action. December 2020, it allocated $13 million to fund\nactivities that required a longer lead time and must\nIn 2020, OCHA\u2019s Centre for Humanitarian Data happen early, such as preparing land and planting\nsupported Ethiopia\u2019s humanitarian community drought-resistant crops. It then released a second\nto develop a trigger for drought as the basis for allocation of $7 million in February 2021, once the\nthe framework. The trigger would be activated likelihood of below-average rains was confirmed.\nif climate forecasts and seasonal risk analysis\nsuggested below-average rainfall during the coming At a time when Ethiopia had already experienced\nrainy season, and if food security conditions were one failed rainy season, CERF\u2019s funding assisted\nexpected to deteriorate based on projections from more than 900,000 people at a critical time, ena-\nthe Integrated Food Security Phase Classification.3 bling communities to make better choices for their\nIf the trigger was activated, CERF would provide families and supporting livelihoods before it was\n$20 million to humanitarian partners for a set of too late. Unlike in previous droughts, almost half of\npredetermined actions. the recipients (44 per cent) reported that the assis-\ntance came at the right time.4\nOn 28 October 2020, the UN Humanitarian Country\nTeam endorsed the framework. However, just days Tens of thousands of households received short-cy-\nlater, Ethiopia\u2019s Tigray crisis consumed the Govern- cle/drought-tolerant seeds, livestock packages\nment, the humanitarian community and political that included supplemental feeding, treatment and\nattention. Dry conditions in south-east Ethiopia vaccination services, and unconditional cash.5 Re-\nand forecasts of another failed rainy season in cipients nearly unanimously agreed the assistance\nearly 2021 were overlooked among the noise of the was useful.6\nescalating conflict. But they were not overlooked\nby the humanitarian community. It had resources As one recipient noted: \u201cMy family benefited from\nand an agreed plan to translate early warning into the assistance, as we used the money to buy food\naction, thanks to the anticipatory action framework items. In addition, we spent the money to take care\nin place. of our children, and the animal feed support was\nalso helpful for our livestock, as there was inade-\nBy late November 2020, four regions in Ethiopia quate pasture.\u201d Food and Agriculture Organization\nmet the food insecurity criteria. Almost 13 million anticipatory action recipient.7\npeople were projected to experience crisis or emer-\ngency levels of food insecurity within six months\n3 The IPC Acute Food Insecurity classification differentiates between different levels of severity of acute food insecurity. IPC3+ refers to the crisis\nphase, characterized by a significant lack of food access, high levels of acute malnutrition, and excess mortality.\n4 Ethiopia survey report by 60 Decibels.\n5 CERF Anticipatory Action Report 2021.\n6 E. Easton-Calabria et al, Anticipatory Action in Complex Crises: Lessons from Ethiopia (December, 2023).\n7 E. Easton-Calabria et al, Anticipatory Action in Complex Crises: Lessons from Ethiopia (December, 2023).\n6\nOn 10 May 2022 in the internally displaced person (IDP) site of Guyah, Afar Region, Ethiopia, 13-year-old Keria (left) and another girl, fetch water from a\nUNICEF supported water point. Credit: UNICEF/ Sewunet\nBy utilizing existing community-based struc- To prevent 13,000 children from dropping out of\ntures \u2013 such as religious institutions, kebele (small school, almost 3,000 households \u2013 mostly female\nadministrative unit) leaders, marketplaces, school headed \u2013 received cash or vouchers to support\nclubs and local radio stations \u2013 almost 250,000 education costs. A follow-up survey indicated that\ncommunity members learned about drought-related 100 per cent of the respondents used the cash to\nprotection risks.8 buy school materials. In another survey, 90 per cent\nof the respondents stated the cash helped ensure\nMore than 80,000 people were able to access safe their children\u2019s continued access to education.11\ndrinking water through the early rehabilitation of\nboreholes and non-functioning water schemes Overall, anticipatory action helped to prevent\nand water piping, including in health facilities and drought-related suffering. A satisfaction survey\nschools. A further 25,000 children received water in conducted by 60_decibels found that three out of\nschool during the drought.9 five recipients noted an overall improvement in their\nquality of life due to their increased ability to afford\nAccess to clean water meant less disease. As one food, livelihood inputs and household bills.12\nrecipient observed: \u201cThe aid improved and secured\nour family hygiene and health. We\u2019ve been able to\nprevent communicable diseases.\u201d United Nations\nPopulation Fund anticipatory action recipient.10\n8 CERF Anticipatory Action Report 2021.\n9 CERF Anticipatory Action Report 2021.\n10 Ethiopia survey report by 60 Decibels.\n11 CERF Anticipatory Action Report 2021.\n12 CERF Anticipatory Action Report 2021.\n7\nTakeaway\nFor Ethiopia, living with climate volatility will require CERF\u2019s $20 million grant was approximately 1 per\nanticipating droughts and floods, and mitigating cent of the overall requirements of the Humanitar-\ntheir impact on the most vulnerable people. Ulti- ian Response Plan in 2021, and funding require-\nmately, more climate adaptation is required, but the ments for the 2020-2022 drought response were\nhumanitarian community also has an important only half met.14 It is therefore understandable that\nrole to play. The pilot in Ethiopia demonstrates that 49 per cent of the recipients interviewed in 2022\nanticipatory action holds promise. said the anticipatory action assistance hardly met\nor met none of their drought-related needs.15\nIn a world where humanitarian crises compete for\npolitical attention and resources, Ethiopia illustrates As Lisa Doughten, Director of OCHA\u2019s Financing and\nthat anticipatory action frameworks make acting Partnerships Division, noted: \u201cCERF funding contin-\nearly the default. ues to be an indispensable enabler for anticipatory\naction, but to truly scale up its impact, more flexible\nCatherine Sozi, the former UN Resident and Human- and coordinated financing is needed.\u201d\nitarian Coordinator for Ethiopia, explained: \u201cIn late\n2020 and 2021, media headlines, political atten- In addition to exposing the impact of funding\ntion and resources were focused on the COVID-19 constraints, the pilot also yielded lessons in how to\npandemic, the Tigray crisis in Ethiopia, a military increase the impact of anticipatory action.\ncoup in Myanmar, the Taliban takeover in Afghani-\nstan and a large-scale earthquake in Haiti. No one Julia Wittig, Anticipatory Action Lead for CERF,\nwas paying attention to failed rains in Ethiopia. The explained: \u201cThe pilot helped us to further refine the\nanticipatory action framework meant we could act type of activities and the timing of those interven-\nwhen it would make a difference to communities. tions. For example, some activities, such as prepar-\nWe didn\u2019t need to wait for people to suffer and the ing land and distributing seed, must happen early\ndrought to make the headlines.\u201d and require a longer lead time. But they are critical\nto food consumption and preventing negative cop-\nThe pilot also revealed valuable lessons about ing strategies later. The decision to split the CERF\nanticipatory action\u2019s place in the context of pro- allocation allowed us to time interventions for when\ntracted and overlapping crises and an underfunded they make the difference, while mitigating the risks\nresponse. of releasing funding before a drought is certain.\u201d\nAnticipatory action is intended to mitigate the im- Improving collective targeting can also help to\npact of one shock, such as drought. CERF funding maximize impact. Humanitarian partners jointly\ncame after the first failed rains (October-December identified districts for assistance, but aid was often\n2020), but households had to endure five consec- distributed to different households with different\nutive below-average rainy seasons (October 2020 packages of support. The 60_decibels survey\nto June 2022).13 Communities also had to live with illustrated a more granular approach, where human-\ndrought alongside other crises, such as conflict, itarian partners identify the most vulnerable house-\nlocalized flooding, a locust invasion and inflation. holds to assist with an intersectoral package of\nsupport. This could have led to better outcomes for\nIt\u2019s also important to consider the scale of antic- those families.16 OCHA and humanitarian partners\nipatory action compared to the size of the need. are pursuing collective targeting for anticipatory\n13 Ethiopia survey report by 60 Decibels.\n14 OCHA, Financial Tracking System. Note, the 2021 Ethiopia Humanitarian Response Plan was not exclusive to needs generated by the drought but\nwas a major component. The 2022 Ethiopia Humanitarian Response Plan was in response to the drought and the Tigray crisis.\n15 Ethiopia survey report by 60 Decibels.\n16 Ethiopia survey report by 60 Decibels.\n8\naction frameworks in Bangladesh, Burkina Faso, action frameworks in Burkina Faso, the Central\nChad and Niger. American Dry Corridor, Chad and Niger.\nEthiopia also helped to improve future anticipatory Overall, the pilot successfully mitigated some of the\naction triggers for droughts. Ethiopia shows that drought\u2019s impact on vulnerable households. Impor-\nfood security assessments are crucial to determine tantly, it proved that anticipatory action works, gen-\nthe population\u2019s vulnerability and where to prior- erating momentum in the humanitarian community\nitize aid. However, the assessments require some to get ahead of climate shocks. OCHA will update\nimprovements to serve as a trigger for anticipatory and renew Ethiopia\u2019s anticipatory action framework\naction. in 2024, with CERF pre-committing $15 million.\nWillem Muhren, Head of OCHA\u2019s Information Man- Given the drought pilot\u2019s success, and recognizing\nagement Unit in Ethiopia, said: \u201cFood security as- the multiple risks Ethiopia faces, a new collective\nsessments are published after the rainy season so anticipatory action framework for floods will be\nthey can estimate what will happen with the food explored. This is part of OCHA\u2019s portfolio of pre-ar-\nsecurity situation. This is too late, for example, if we ranged funding of more than $100 million in 15\nwant to plant drought-resistant seeds. We need to countries for storms, floods, droughts and cholera.\nget ahead of the shock, not just respond earlier.\u201d\nAs Ethiopia confronts another drought in 2024,\nSince the pilot in Ethiopia, OCHA is focusing on more flexible funding for anticipatory action is\nclimate indicators for drought-related anticipatory needed. As Ms. Msuya concluded: \u201cnow is the time\nto scale.\u201d\nOn 12 May 2022 at a UNICEF MY Home/Bete school Dubti, Afar Region, Ethiopia, Ibrahim Ali (left) attends a class designed to provide a safe space for\nboys and girls living amid humanitarian emergencies by integrating accelerated learning, child protection and skills development. Credit: UNICEF\n9",
    "source_file": "pdf3.txt"
  },
  {
    "chunk_id": "pdf1_0",
    "text": "QUALIT Y MEASURES FOR",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_1",
    "text": "HUMANITARIAN DATA",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_2",
    "text": "SPRINT REPORT",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_3",
    "text": "APRIL 2023",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_4",
    "text": "QUALIT Y MEASURES FOR",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_5",
    "text": "HUMANITARIAN DATA",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_6",
    "text": "SPRINT REPORT",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_7",
    "text": "APRIL 2023",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_8",
    "text": "MEET THE TEAM\nLead Technology Research\nKasia Chmielinski Matt Taylor Sarah Newman\nDesign Design\nataD\nJessica Yurkofsky Chelsea Qiu\nnairatinamuH\nrof\nserusaeM\nytilauQ\ni\nI.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_9",
    "text": "BACKGROUND\n1\nII.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_10",
    "text": "CHALLENGES\n2\nIII.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_11",
    "text": "KEY FINDINGS\n4\nIV.\nAPPROACH & DIRECTIONS\n6\nV.\nRECOMMENDATIONS & DESIGNS\n11\nVI.\nROADMAP & IMPLEMENTATION\n15\nVII.\nataD",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_12",
    "text": "ADDITIONAL CONSIDERATIONS\nnairatinamuH\n19\nVIII.\nrof",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_13",
    "text": "CONCLUSION\nserusaeM\n21\nytilauQ\nIX.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_14",
    "text": "APPENDIX\n22\nii\nI.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_15",
    "text": "BACKGROUND\nGoals\nThe purpose of this Data Labeling project was for select members of the Data Nutrition\nProject team to research and prototype possible quality measures for humanitarian\ndatasets that are hosted on the HDX platform, which is owned and managed by the UN\nCentre for Humanitarian Data. The scope included:\n\u2022 User and Platform Research. We conducted user research with the Centre\nteam (and additional stakeholders suggested by the Centre) to learn about 1)\nDifferent conceptions of data quality in the humanitarian sector; 2) How users\nfind and select data on HDX, including priority of criteria; 3) The current DPT /\nHDX team QA workflow with regards to assessing data quality.\n\u2022 Quality Measurement Prototype. Building on user research and an\nassessment of the state of the data and the needs in play, and using two\npreselected datasets as examples, we prototyped a quality measures label for\nHDX. The prototyping involved varying degrees of fidelity and was shaped by\nfeedback from the Centre team.\n\u2022 Preliminary thoughts on Scalability. Through research and prototyping,\nwe began to explore how this effort could scale, including paths toward\nautomatability. Our findings are discussed in this report.\nPhilosophy\nThe Data Nutrition Project is a non-profit initiative that formed in 2018 to develop tools\nand practices to improve transparency into datasets. Our team is interdisciplinary,\nand we leverage insights from a variety of fields, including product development, data\nscience, ethics, engineering, design, and education. Our approach with our Nutrition\nLabels for Datasets is threefold: 1) We encourage the creation, documentation, and\nataD\npublishing of higher quality data; 2) We enable transparency into datasets through our\nnairatinamuH\nlegible, extensible, interactive framework; and 3) Our Labels provide education about\nwhat kinds of information a user should ascertain before using a dataset. We bring\nthis approach into our work with clients, where we prioritize user-centered design,\nrealistic goals, and practitioner-focused outcomes, informed by our experience working\nrof\nin data transparency initiatives and with the real tradeoffs and tensions faced by data serusaeM\npractitioners. In seeking to do work that is both applied and realizable, we aim to\nprovide not only a long-term vision but also a roadmap with recommendations for future\niterations of a project. ytilauQ\n1\nII.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_16",
    "text": "CHALLENGES\nThere are many challenges that can be impediments to dataset quality. This is certainly\nthe case in the humanitarian sector, where crises unfold quickly and data capture will\nalmost always be imperfect, often as a consequence of the need for rapid collection.\nFurthermore, on a more philosophical level, the assigning of rankings, scores, or grades\nto a dataset will always be tricky business, for the legitimacy of the scoring standards\nthemselves can undermine the effort for scoring in the first place. We believe it is useful\nto explicitly enumerate these challenges before we describe our recommendations. The\nlatter were formed in light of the former, which will be familiar to the HDX team and to\nothers who have worked on dataset metrics, measures, and assessments.\nChallenge 1 - Identifying scoring methods that are succinct while not overly simplistic\nScores are meant to provide information quickly and ease comparison, while inviting\nfurther exploration. They can, however, risk being reductive or overly simplistic. This is\nparticularly difficult when comparing datasets whose provenances are entirely different.\nA score that is too simplistic will not only be useless but may also seem arbitrary. A\nsingle score to compare across inconsistent data types or domains may risk both.\nDepending on the scoring framework, there is the additional challenge of validating\naccuracy: what is the rubric by which this score was determined? How is accuracy of\nevaluation defined and ensured?\nChallenge 2 - Balancing scalable (quantitative) & comprehensive (qualitative) measures\nQualitative information helps mitigate some of the concerns above, as it is often more\ncontext-aware than quantitative statistics alone. However, qualitative information\n(such as detailed provenance information) is also resource-intensive to collect, often is\ndomain-specific, and is sometimes impossible to obtain, such as when provenance is\nsimply unknown. Conversely, quantitative measures can be easier to automate and thus\neasier to scale, but they can miss nuance or context that is essential to understanding\nthe particulars of a dataset. The tension here is between usefulness and scalability; in ataD\nour experience, this is the most common challenge in dataset transparency efforts.\nnairatinamuH\nChallenge 3 - Communicating quality to motivate rather than disincentivize\nrof\nOur hope is that quality measures will, in the short term, facilitate better data use\nserusaeM\nchoices, and in the long term motivate the creation and publishing of better quality\ndata by changing user expectations and data collection habits. However, depending on\nhow measures are disclosed and how scores are determined, they could discourage\nytilauQ\nfull transparency when sharing data in cases where increased transparency might\nnegatively affect a score. The challenge here is to motivate better quality data without\npenalizing or disincentivizing current dataset owners from sharing data or disclosing\nshortcomings. For example, over the course of our research, a data organization voiced\nconcern that their data was being marked \u201cincomplete\u201d even though it was \u201cas good as 2\nit possibly could have been\u2019\u2019 given a particular set of circumstances. It is important to\nnote that in some scenarios certain information cannot be ascertained and this should\nnot reflect negatively on the quality of the dataset.\nChallenge 4 - Building a quality framework that balances flexibility with consistency\nAs the humanitarian sector changes over time with respect to crises and data needs,\nany discrete quality metrics will also likely change. For these reasons, whatever is built\nwill need to be adaptable. However, consistency is also important, so that datasets from\ndifferent time periods can be compared, and so that dataset owners and site visitors can\ndevelop familiarity and comfort with the site. For example, when and how do existing\ndatasets get re-evaluated under updated scoring rubrics? What is the right approach\nto keeping information up to date (timely and punctual) that will be both robust and\nscalable across thousands of datasets? When or how will that score be altered or\ndowngraded as time goes on?\nChallenge 5 - Determining responsibilities within the data pipeline\nHDX is committed to hosting good quality data, and requires QA and other data\nonboarding processes. However, HDX, like all organizations in the humanitarian sector,\nhas limited resources, with respect to time and personnel for validating datasets.\nFurthermore, even if there were no resource constraints, there are always knowledge\ngaps between on-the-ground domain expertise and data experts looking at raw or\nprocessed data. This challenge is shared among all data validation efforts, and it\nmight be even more drastic in the domain of humanitarian data, where data collection\nmethods require agility, and thus context-awareness and domain knowledge is essential\nfor accurately interpreting or validating the data. However, HDX is extremely well-\npositioned to build a process, and for that process to incorporate shared responsibility\nfor data-validation, as we discuss below.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n3\nIII.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_17",
    "text": "KEY FINDINGS\nThe five-week sprint gave rise to a number of key findings, which informed our final\ndesigns and recommendations.\nFinding 1 - HDX is best positioned to define rather than assess quality\nDue to the range of dataset types on HDX, limitations of domain knowledge, and\nresource constraints, HDX is not well positioned to conduct quality assessments\nfor all datasets on its platform, and instead, HDX should focus that attention to\ndata types or categories that have been defined as critical, such as the Data\nGrids. HDX is best positioned to\n1) Define the framework for \u201cquality\u201d of datasets on HDX (meaning define\nwhat is getting collected and assessed);\n2) Facilitate the gathering of this information from data organizations;\n3) Provide a display of this information to data users on HDX, designed\nin alignment with user needs.\nFinding 2 - There is an opportunity to leverage existing quality measures\nData quality assessment is already conducted on HDX, albeit at different times\nand displayed or communicated in disparate regions of the site. This provides\nan opportunity, as a first step, to aggregate, prioritize, and organize information\nthat has already been gathered. Further ambitions to collect and validate data\nthat is not currently collected should come only after the current effort to bring\ntransparency and legibility to the valuable information that HDX already collects.\nIt is our belief that a phased approach, starting with the information already\ncollected, and expanding out from there to leverage the credibility of data\nataD\norganizations, will be most suitable to HDX\u2019s current infrastructure.\nnairatinamuH\nFinding 3 - Domain experts and third-party validators can provide rof\ncomplementary value serusaeM\nDue to the variety of data types and domains on HDX, data content quality\nassessment is likely to require verticalization (e.g. different approaches for GIS ytilauQ\ndata, CODs, etc). Considering the ranges of domain expertise required, and\nthe limited bandwidth on the Centre team, our recommendation, as stated in\nFinding 1, is that the Centre define the parameters of \u201cquality\u201d through a set\nof extended metadata that is to be displayed HDX, and then work with data\n4\nowners and third-party assessors who will be the ones to contribute much\nof this extended metadata. This would follow HDX\u2019s work to aggregate the\ninformation already collected, and determine the best design approach for\ncommunicating this information to users. Many data organizations have quality\nframeworks or assessments for their own data, and these could be indicated on\nHDX to communicate things like known issues and certain strengths of a dataset\nwithin its domain. Working in collaboration with these organizations can lend\ninstitutional validation to these quality frameworks \u2013 which, either independently\nor alongside additional support, could motivate third parties to work with HDX \u2013\nand would provide domain expertise to HDX and its users. Using these external\nthird-party-determined metrics also encourages other organizations to consider\nadapting their use frameworks to include quality, which can drive cultural change\naround responsible data usage.\nFinding 4 - Automation of QA and assessment tasks can enable scale\nWhile there is some initial groundwork required to prepare for automating certain\nQA tasks, we believe that HDX investing the time to move in this direction will,\nin the long run, enable quality measures to be assessed and applied at scale.\nFor example, automating the collection of certain metadata (such as restrictions\naround API use, required metadata through API collection, or requiring the use of\nHXL) can make it easier to enforce and collect technical quality information about\ndatasets. We know that HDX has done a lot of work toward formatting datasets\ninto HXL and advocating for others to do the same. We understand that this is\nno small feat, but if HXL can be more widely adopted, this would enable greater\ninteroperability, comparability, and analysis.\nFinding 5 - Primary use case is data selection, which can ultimately be\nataD\nsupported through metrics comparison\nnairatinamuH\nThrough our conversations and interviews, we learned that the primary reason for\nhaving quality measures included on the platform is to improve dataset selection,\nwith additional use cases including dataset comparison (choosing among\nseveral on HDX, wanting to quickly ascertain which is better for a certain need) rof\nor dataset combination (merging several to cover a bigger geographic region serusaeM\nor to otherwise extend a particular dataset). The availability of legible, digestible\nmeasures, such as format, update frequency, and uses and restrictions, permits\nytilauQ\nusers to quickly scan for their particular needs. We recommend prioritizing\ninformation that is already available, and in later phases collecting (and\ncollaborating with others to collect) additional information that can bring even\nmore value to dataset users.\n5\nIV.\nAPPROACH & DIRECTIONS\nWe explored quality measures on the HDX platform through a tailored 5-week discovery\nsprint in a three-part inquiry: 1) Researching data quality principles in the context of\nhumanitarian data and the existing platform; 2) Developing several prototype directions\nbuilding on this principles-based foundation drawing on our expertise in data quality\n\u201clabels\u201d; 3) Refining our direction and recommended implementation strategy based on\nfeedback from the Centre.\nResearch\nOur team spent the first two weeks researching background materials and interviewing\ndiverse stakeholders. We read and analyzed approximately a dozen reports about data\nquality principles in the humanitarian sector (published by HDX, UNICEF, UK AID, IOM,\nOCHA, ICRC, GAHI, DSEG, GSQAF, including others) in order to build an understanding\nof quality measures on the HDX platform. We sketched a matrix aligning and comparing\nprinciples across several major organizations (compared primarily to GDQAF principles,\nwhich were most commonly cited as baseline) in order to better understand these\nconcepts in the context of HDX (fig. 1). Notably, we saw a gap in the literature published\nby HDX around \u201ccredibility,\u201d so we sketched this into the matrix (in light blue).\nIn parallel, we conducted interviews with stakeholders that represented key points along\nthe data collection, processing, hosting, and use timeline, including data partners (IOM,\nHumanitarian OpenStreetMap Team) and several within the Centre (Data Partnerships,\nData Responsibility, organization onboarding, product development, quality assessment\nprocess, and others), in order to understand how the Centre thinks about quality, to\nlearn about existing mechanisms for identifying and surfacing quality issues, and to\nexplore future scenarios for expanding or adjusting quality assessment practices.\nataD\nnairatinamuH\nrof\nserusaeM\nFigure 1. Matrix aligning data ytilauQ\nquality principles across several\norganizations, with HDX in blue.\nFull size matrix included in\nappendix.\n6\nPrototype directions\nBased on the research and interviews conducted in the first two weeks of our sprint,\nthe DNP team developed four potential paths forward to highlight specific data quality\nprinciples. These were:",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_18",
    "text": "1. Comparability: features to support dataset selection\nThroughout the interviews, we heard that a primary use case for data quality\nassessment on HDX (and more broadly in the sector) was the enabling of dataset\nselection, either for a particular need at hand, to join with other external or proprietary\ndata (e.g. Combine several datasets about a particular geography), or to compare\nagainst similar datasets to assess which is best aligned for a particular use (e.g.\nIdentifying which administrative boundary dataset is appropriate if there are several to\nchoose from). To support this particular use case of dataset selection, which we felt was\nbest aligned to the data principle of comparability, we proposed features that support\nthe direct comparing and contrasting of datasets based on metadata comparison (fig. 2)\nFigure 2. Prototype\nsketches of features to\nsupport the comparing and\nataD\ncontrasting of metadata on\nnairatinamuH\nsimilar datasets.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_19",
    "text": "2. Credibility: leveraging trust in organizations\nrof\nserusaeM\nFrom the earliest conversations, data organizations and the Centre teams stressed the\ncritical dependency between data quality and data collection and processing practices.\nIn fact, although there were several measures and principles of data quality that came\nytilauQ\n\u201cafter\u201d the processing of the data \u2013 such as freshness, accessibility, and interpretability \u2013\nsome of the most salient measures could only be assessed by those most familiar with\nthe origin of the data itself (fig. 3). This highlighted not only the importance of but also\nthe reliance upon data organizations to help surface data quality on HDX.\n7\nFigure 3. Aligning the data pipeline to quality principles and responsible parties highlights the critical\nrelationship between the data organization and HDX for the assessment and communication of data\nquality.\nOur second prototype aimed to leverage trust in organizations based on their data\npractices and commitment to data quality as a proxy for the data quality principle of\ncredibility. For this approach, our recommendation (which follows the initial work\nperformed by DNP team member and 2021 Strategic Communications Data Fellow\nKasia Chmielinski), HDX would build a framework for organization trust \u201clevels\u201d\nthat signal an active, relational approach towards quality: one that depends on the\norganization that produces or publishes the dataset. Datasets would then qualitatively\ninherit the credibility from the organization that produced them, with the organization\nserving as a proxy for responsible data collection and processing practices (fig. 4).\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\nFigure 4. Prototype sketches of a dataset with a\n\u201clevel 3\u201d trust organization indicator as a proxy\nfor the data quality principle of credibility.\n8",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_20",
    "text": "3. Completeness, timeliness: assessing metadata completeness\nThe last two prototype directions are related to fitness for purpose - assessing whether\nwhat is represented in the dataset is appropriate for use (complete, timely, relevant,\naccurate). We found fitness for purpose metrics to be the most challenging due to the\ndistribution of responsibility in dataset management, and the realities of data collection\nin humanitarian situations. Stated simply, it is very hard to assess quality without an\nideal \u201cground truth\u201d dataset against which to compare, or without access to information\nabout the collection and processing practices of the data owner. To facilitate our\nanalysis, we approached these measures along two axes: the assessor (data owner vs.\nthird party), and the type of assessment (qualitative vs. quantitative) (fig. 5). The resulting\nmatrix helps clarify four directions: qualitative (not easily scorable) assessment at two\nlevels of granularity, where the data owner can be much more detailed than a third\nparty, and two quantitative (more easily scored or ranked) approaches, one domain-\nspecific (e.g. GIS data quality, ranked by the data owner) and the other focused on\nadherence to a metadata or technical standard (e.g. metadata completeness, in this\ncase conducted by HDX).\nBuilding off this analysis, our third prototype direction falls within the third-party\nquantitative assessment for completeness and timeliness: measuring and reporting\nadherence to standards of metadata completeness. This version, along with the former\n(Credibility of Trusted Orgs) and the one that follows, combine for our recommendation\nto the Centre team. In this version, HDX builds a framework of expectations of metadata\nstandards and provides a score or indicator on whether a dataset meets that standard.\nFor example, HDX could incentivize higher data quality through a measurement that\nassigns a higher score (or ranking) for the use of automatic data submission, the use of\nstandard data elements (such as P-codes or HXL), or attestation of a third party review.\nFor simplicity of communication, these metadata could be categorized into sections\n(such as \u201cTrust,\u201d \u201cContent Quality,\u201d \u201cUses,\u201d etc.) (fig. 6).\nataD\nnairatinamuH\nrof\nserusaeM\nFigure 5 (left). Quadrant analysis of fitness for purpose\nmeasures along two axes: responsible party (data owner / ytilauQ\nthird-party) and type of measure (qualitative / quantitative).\nFigure 6 (right). Prototype for a \u201cquality label\u201d that\nassesses and highlights metadata completeness across a\nnumber of categories and against a common framework.\n9",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_21",
    "text": "4. Relevance, accuracy: assessing fitness for purpose\nThe final prototype approach, assessing fitness for purpose in alignment with data\nprinciples relevance and accuracy, most likely requires direct input from the\ndata owner or a party that is familiar with the entire lifecycle of data collection and\nprocessing (see the left hand side of the quadrant matrix fig. 5). This is because it is\nextremely difficult, if not impossible, to understand the quality of content contained\nwithin a dataset without understanding the context in which it was gathered, processed,\nand how it will be used. This requires both knowledge of the data collection process as\nwell as significant domain expertise.\nOur suggestion for this approach is thus to acknowledge the dependency between\nHDX and the data owner (organization) and leverage structured frameworks for the\ncommunication of dataset quality information. For example, HDX could partner with\nthird party organizations that certify certain data quality domains (e.g. GIS data, CODs,\nboundary data, education or health data) and when datasets achieve that certification,\nHDX could communicate that information on the platform. In some instances, HDX\nmay also be a certifier \u2013 the Data Grids are a good example of this \u2013 but for the sake of\nscaling, expanding to third party certifications will be less resource-intensive and more\napplicable to the breadth of datasets within the HDX platform. An additional option\nmight be for HDX to work with particular data organizations to surface their internal\nquality measures through standardized metadata fields that, while not consistent with\nrespect to content across data organizations, would be a consistent field that appears\non all datasets regardless of organization or domain (e.g. a \u201cdomain-specific quality\nmeasure\u201d field that could be defined differently across organizations and domains). For\nexample, in the figure below (fig. 7), HDX has defined a portion of the metadata structure\nto include both a \u201cself-reported QA adherence\u201d from the Data Organization as well as a\n\u201cdomain-specific metric\u201d built by a third party.\nataD\nnairatinamuH\nFigure 7. Prototype sketch of Content\nrof\nQuality Measures that include quality\nserusaeM\ncertifications from HDX, Data Org\n/ Owner and Third-Party expert\norganizations. These could be binary\nytilauQ\n(pass / fail) or quantitative (e.g. a score).\n10\nV.\nRECOMMENDATIONS & DESIGNS\nRecommendations\nFrom our four prototype directions outlined above, and integrating feedback from\nthe Centre team, we recommend a combination of prototype directions 2, 3, and 4,\norganized into three phases of work. Detailed work plans and sketches for Phase 1 are\nincluded in this report.\nWe also share recommendations for how to approach the continuation of these\nexplorations in Phases 2 and 3, and include a preview (low fidelity sketch) of how\nextended information that might be included in Phase 3 could appear on HDX. The\nphased approach enables HDX to prioritize the information that is already available,\nwhile beginning to build an on-ramp for further work that will require additional\nresources and infrastructure. A summary of the phases is included below, and discussed\nin more detail in Section VI.\nPhase 1 - Fully scoped, ready to implement\nAggregate and make easily accessible the content that HDX already collects with\na specialized Quality Measures pane on the HDX site. This Quality Measures\npane is divided into four parts: Use, Trust & Safety, Content Quality, and\nTechnical Specs. The measures in each section are summed, not as a score, but\nas an indicator for comparability, and an indication of the value of transparency\ninto dataset information. These sums are visible in the search view on HDX as at-\na-glance indicators about dataset quality measures.\nPhase 2 - High-level spec, requires additional research and design\nCreate an organization review and vetting process that allows for trusted orgs to ataD\nserve as a proxy \u2013 or at least an additional indicator \u2013 for dataset credibility, and\nnairatinamuH\nintroduce org badges into the Quality Measures pane; automate QA processes\nwhere possible; begin research into domain-specific quality measures.\nrof\nserusaeM\nPhase 3 - High-level spec, requires additional research & design\nCollect additional quality measures through third-party organization QA ytilauQ\nprocesses; introduce self-reporting assessment for domain-specific quality\nmeasures; implement dataset comparison suggestions of \u201csimilar datasets,\u201d and\nconsider adding further measures pending user feedback from Phases 1 and 2.\n11\nAdditional notes on our recommendations:\n\u2022 Our approach involved prototyping on real datasets. This proved to be an\nessential part of our process; it enabled our work to be grounded in the\nparticulars of the HDX context. We will follow this model for any future work\nwith HDX.\n\u2022 Based on our research and interviews, completeness, accuracy, and relevance\nwere the quality measures that were most useful, and HDX already collects\ncertain information that has indicators for these areas. The work in Phase 1\ninvolves organizing this information into a legible knowledge structure, and\ndesigning it in a visible and easily accessible way. The chart in Section VI below\n(fig. 12) Indicates where and when this information is collected, and possible\nanswers for each field.\n\u2022 Based on the needs of HDX users and the circumstances \u2013 20K+ disparate\ndatasets, limited resources for additional dataset security \u2013 we determined that\nHDX ought to build frameworks for measuring metadata completeness and,\nadditionally, seek out quality certifications (likely conducted by third parties, but\ncould also be conducted internally by HDX).\nDesigns\nDNP created three Phase 1 designs (a template + two dataset-specific versions), a\n\u201csearch\u201d view, and a Phase 3 preview of the dataset comparison section. Full resolution\ndesigns are included in the Appendix.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\nFigure 8. Phase 1 content in Quality Measures Pane on HDX site. 12\nFigure 9. HDX search view with Quality Measure counts.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n13\nFigure 10. Views of Quality Measures Panes (Phase 1) for two example datasets.\nFigure 11. \u201cSimilar Datasets\u201d comparison sketch, to be refined in Phase 3.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n14\nVI.\nROADMAP & IMPLEMENTATION\nPhase 1\nApproach\nWe recommend creating a new Quality Measures pane (alongside the Data &\nResources and Metadata panes) for the HDX dataset landing page. This view will\nconsolidate existing measures from across HDX into one view that gives users a quick\noverview of dataset information and proxies for quality, organized into four sections:\nUse | Trust & Safety | Content Quality | Technical Specs\nThe technical needs for this should be minimal, and implementation process could\ninclude: 1) Implementing the prototype on HDX\u2019s staging server; 2) Soliciting feedback\non the prototype from key stakeholders, integrating feedback where possible; and 3)\nRolling out the measures design to HDX. Because all of the information present in the\nPhase 1 prototype is already collected, it would be a matter of consolidating information\nrather than creating new information or processes. Below (fig. 12)is a chart of the\ninformation contained in the Phase 1 designs, and where that information comes from in\nthe HDX workflow.\nTechnical Considerations\nPhase 1 involves no database changes, and minimal back-end implementation. It will\nfocus primarily on front-end implementation for HDX, and soliciting feedback from\nstakeholders.\n\u2022 Database/DevOps. The data needed for the Phase 1 measures view exists\nwithin HDX already, meaning there should be no database migration/change\nconsiderations.\n\u2022 Back-end. Implementation would be focused on making sure all of the\nnecessary quality measures are available to the front-end website. While many\nof these are already available to the front-end, such as last-updated date and ataD\ncaveats, some things may not be readily available, such as whether a QA\nnairatinamuH\ncheck has been conducted. This would involve an audit of existing data access\nendpoints, with the potential need to implement a few new back-end endpoints\nfor existing information. It may also require reformatting some of the back-end\nendpoints to ensure the information needed by the front-end is in the right format. rof\nserusaeM\n\u2022 Front-end. Front-end work would be the most significant portion of this phase. It\nwould include implementing back-end requests and an accessible, localizable UX.\n\u2022 Feedback. It will be important to have a clearly defined list of stakeholders ytilauQ\nfrom whom to get feedback on Phase 1 designs, and clear reasons for why\nthey are being chosen, and the kind of feedback sought. It will be important to\nset limitations for the scope of their feedback, so that they understand what is\npossible in Phase 1.\n15\nPhase 1 is intentionally a consolidation and \u201csurfacing and organizing what is already\nthere\u201d phase and thus should not require substantial technical work.\nQuality Section Measure HDX Process/source Response Parameters\nUse NEW: Dataset Upload -\nIntended Use Open Text\nIntended Use Field\nDataset Upload -\nRestrictions Open Text\nCaveats\nTrust & Safety PII HDX QA Yes, No\nSDC Check HDX QA Yes, No\nPassed HDX QA HDX QA Yes, No\nContent Quality Update Frequency &\nLast Updated date* Frequency - Multiple choice\nDataset Upload -\n(Every day, every week, every\nExpected Frequency\n*When these two pieces of two weeks, etc.)\nUpdate, Upload\ninformation conflict, it should\nbe noted as conflicting timestamp\nLast uploaded- Date-time\ninformation, and the more\nrecent one gets prioritized.\nMultiple Choice (Census,\nDataset Upload - Sample Survey, Direct\nCollection Method\nMethodology Observational Data, Registry,\nOther)\nMultiple Choice (national or\nLevel of Analysis HDX QA\nsub-national)\nBadge (i.e. it is present if the\nreview has been done, absent\nif not).\nReview by HDX Data\nQuality Certifications\nTeam Possible badges include:\nDatagrid Dataset (NEW, would\nhave to be imported through a\nscript), COD\nTechnical Specs Dataset Upload - Badge (i.e. it is present if\nP-Code ataD\nAutomatically reviewed P-codes are used, absent if not).\nDataset Upload - Badge (i.e. it is present if HXL is nairatinamuH\nHXL\nAutomatically reviewed used, absent if not).\nValid URLs HDX QA Yes, No\nNEW: Dataset Upload -\nAPI Used Yes, No\nAutomatically reviewed rof\nSelect All that Apply (.csv, .kxl, serusaeM\nDataset Upload -\nFormat .xlsx, etc. [Centre to generate\nResource Upload\ncomprehensive list])\nytilauQ\nFigure 12. Phase 1 Quality Measures table including source of information and response parameters.\n16\nFurther recommendations\nPhase 2\nPhases 2 and 3 research and designs could be explored in future engagements.\nApproach\nPhase 2 has three specific foci: 1) determining measures for and assessing\norganizational trust, 2) creating capacity for longer-term quality measures development,\nand 3) research on domain-specific measures. For organizational trust, the goal would\nbe to come up with an architecture for measuring an organization\u2019s data processes,\nwhich would be used as a proxy for the credibility of the organization\u2019s datasets. This\nwould then be easily viewable as a quality indicator. The element of creating capacity\nfor QA automation would include identifying the components of the QA process that\nare automatable, and implementing changes in technical workflows to create this\nautomation. The third component of this phase involves researching domain-specific\nquality measures. For example, how is quality evaluated for GIS infrastructure datasets,\nor food security datasets? Domain-specific quality measures will add value to dataset\nreview and may reveal potential avenues for automated assessment.\nTechnical Considerations\nFor each of the three components of Phase 2, technical considerations would\nvary based on the capacity of HDX. A narrative exploring the range of technical\nconsiderations (from limited capacity to high capacity) would be included after\nconducting further research.\nPhase 3\nApproach\nFollowing on the recommendations from Phases 1 & 2, Phase 3 would enable a more\ncomprehensive quality metrics interface. This would include implementing the research\nfrom Phase 2 on domain-specific quality measures and either 1) soliciting third party ataD\norganizations to build certification processes for these metrics, or 2) proposing a self-\nnairatinamuH\nassessment for agreed-upon domain-specific measures.\nWith more comprehensive quality measures, HDX will have the information available for\nmeaningful dataset comparison. This comparison could be displayed in a section called\nrof\n\u201cSimilar Datasets,\u201d enabling users to quickly compare across HDX datasets along the\nserusaeM\nmetrics that are most important for them, and thus choose the best data for their use\ncases.\nytilauQ\nRationale\nIn our research on Common Operational Datasets (CODs) and GIS road-mapping\ndatasets, we discovered that there were some specific methods for quality analysis that\napplied to all datasets in a given domain, but not beyond that domain. CODs have their\n17\nown quality metrics framework and third party certification; GIS road-mapping datasets\ntend to have a standardized structure that most dataset creators use. Consequently, it\nseems feasible that the findings from Phase 2 on domain-specific datasets could be\nrolled into third party certification in some cases, or automated analysis in others (where\ndata is already informally standardized).\nAnd, while the measures in Phase 1 are a helpful starting point to assess quality,\nmore analysis is needed for robust dataset comparison. The addition of third party\ncertifications and automated analyses by domain will provide the content for such\ncomparison. Technical considerations for Phase 3 would be enumerated after the\nresearch, development, and design for Phases 2 and 3.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n18\nVII.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_22",
    "text": "ADDITIONAL CONSIDERATIONS\nImpact on existing data processes and systems\nCurrently, data organizations upload their datasets to HDX either in bulk (using the\nHDX / CKAN APIs) or manually (using the upload form process). Many dataset\nquality measures are already collected during these processes, and we recommend\nthat additional information could be gathered through updating the API and the form.\nCritically, our Phase 1 recommendation does not require this additional infrastructure,\nand instead utilizes only information that HDX already collects. In Phase 2 and beyond,\nthere are additional automated and manual processes in which more metadata is\ngathered, some of which could be leveraged as quality measures. The particulars of this\ninformation - what is gathered and when, and what opportunities there are to collect\nmore data automatically or otherwise \u2013 require further exploration.\nInclusion of quality measures in future technical projects\nOver the course of our engagement with the Centre, we learned that there are\nseveral additional projects in flight that could overlap with the further collection and\nsurfacing of quality metrics on HDX. These projects include the Data-as-a-Service\nwork (ArgoDesign) and shifts towards workflow management of the QA process. It\nis an open question as to how and whether these projects could address the quality\nmeasurements initiative. As these projects get scoped further, we recommend surfacing\nany possible overlaps early and identifying what, if anything, can be added to more\nquickly enable the collection and surfacing of quality measures on HDX.\nTrusted Org & Third Party Certification program definition\nAs outlined above, for Phase 2 would entail research for two initiatives that leverage\nexternal organizations as proxies for credibility: 1) A trusted organization program, which\nresults in a trust score or level that is inherited by all datasets from that organization;\nataD\nand 2) Beginning research on a third party certification program, to be implemented\nnairatinamuH\nin Phase 3, that enables third party validators of content quality (most likely domain\nexpertise) to report whether a dataset meets assessment criteria. Although we\nhave drafted some potential metadata options, including a trust level or score for\norganizations and an area in the UI to hold \u201ccertifications\u201d (which could sit within the\nrof\nquality measurements pane described in Phase 1), further research is required to define serusaeM\nthe set of metadata and the processes of collecting that information; this would include\nstakeholder interviews with current contributing orgs, some of the (informally) trusted\norgs, and additional third party organizations. ytilauQ\n\u201cCompare\u201d feature dependencies\nPhase 3 considerations include the addition of domain-specific metadata, third-party\n19\ncertification metrics, and the ability to compare and see additional, related datasets\non HDX. Multiple conversations with the Centre and its users highlighted the critical\nimportance of data selection. However, the notion of comparing and contrasting\ndatasets requires standardized metadata, some of which is already collected, but\nmuch of which is not programmatically accessible. In particular, the usefulness of the\n\u201ccompare\u201d feature, which requires additional research but could appear, for example,\nwithin the \u201cQuality Measures\u201d tab on the dataset page or the search results returned\nafter submitting a query \u2013 increases significantly with the inclusion of technical\ninformation about the dataset that would be made available through the HXL-ation\nprocess. This is no doubt a challenge, considering that the majority of datasets are not\nyet HXLated. There is an open question about how much metadata must be available\non HDX datasets for the \u201ccompare\u201d feature to be useful.\nResource identification\nEach of the recommendations made in this report will require resources from the Centre\nand, in some cases, beyond the Centre. This echoes a common refrain in technology\nprocesses and projects about resource management, and thus requires consideration\nwithin the context of roadmap prioritization across the larger team. Phase 1 will require\ndesign and technical resources for implementation, though we have tried to scope this\nphase to be relatively small with respect to back-end changes. Phases 2 and 3 will\nrequire additional resources, especially for the trusted org and third party validation\nprograms and UX / Product Management / Development resources for the \u201ccompare\ndatasets\u201d feature set. Additionally, any work with third parties requires not only building\nbut also maintaining relationships over time.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n20\nVIII.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_23",
    "text": "CONCLUSION\nThis report outlines the findings from a five-week research and design sprint,\nundertaken between February into early March 2023, by the DNP team, to deliver\na set of implementable prototypes for quality metrics indicators on the HDX site\n(Phase 1). While the process included contending with some known \u2013 and some new\n\u2013 challenges, we are pleased to share that this sprint has concluded in a number of\nvaluable findings, a concrete set of designs based on content that is already available\non or collected by HDX, a determination of the path forward of a summation \u201cscore\u201d\nthat indicates whether the information is available, rather than an normative grade\non the information itself, and prospects for future work (Phases 2 and 3). We are\nalso delivering complete prototypes of two distinct datasets as an illustration of the\ninformation that can be conveyed in the Phase 1 quality measures pane of the HDX site.\nIn summary, HDX already attends to dataset quality. We found that a lot could be gained\nsimply by consolidating disparate elements from the HDX upload and review process,\nand making this information readily available to dataset users. We also found that\nsumming the information that is provided within four discrete sections (with a numerator\nbut no denominator) enables for a gentle indicator of quantity of information without\npenalizing others for not having that information.\nMany fields that involve data-driven decision making are only now starting to ask\nquestions about dataset quality \u2013 questions that HDX has already answered and started\nto build into its systems. With a concrete, phased approach, HDX can implement quality\nmeasures for data in a way that meets its users\u2019 needs and sets an example for many\nother fields. DNP looks forward to continued collaboration in this process.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n21\nIX.",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf1_24",
    "text": "APPENDIX\nFigure 1. Matrix aligning data quality principles across several organizations, with HDX in blue.\nDocuments cited:\na. USAID, Democratic Republic of Congo, \u201cHow to conduct a data quality assessment (DQA): An\nAid Memoir for a COR/AOR\u201d (March 2012)\nataD\nb. Frontier Technologies Hub, \u201creleasing the power of digital data for development: a guide to new nairatinamuH\nopportunities\u201d (June 2019)\nc. Data Science & Ethics Group, \u201cA Framework for the Ethical Use of Advanced Data Science\nMethods in the Humanitarian Sector\u201d (April 2020) - https://www.hum-dseg.org/dseg-ethical- rof\nframework serusaeM\nd. International Committee of the Red Cross, \u201cHandbook on Data Protection in Humanitarian\nAction. Second Edition\u201d (2020) - https://missingpersons.icrc.org/library/handbook-data- ytilauQ\nprotection-humanitarian-action-second-edition\ne. The United Nations Statistics Division, \u201cGeneric Data Quality Assurance Framework for a UN\nAgency\u201d (September 2015) - https://unstats.un.org/unsd/unsystem/Documents-Sept2015/\nGSQAF-GenericData-Sept2015.pdf 22\nataD\nnairatinamuH\nrof\nserusaeM\nFigure 8. Phase 1 content in Quality Measures Pane on HDX site.\nytilauQ\n23\nFigure 9. HDX Search view with Quality Measure counts.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n24\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n25\nFigure 10a. Views of Quality Measures Panes (Phase 1) for two example datasets - dataset 1 of 2.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n26\nFigure 10b. Views of Quality Measures Panes (Phase 1) for two example datasets - dataset 2 of 2.\nFigure 11. \u201cSimilar Datasets\u201d comparison sketch, to be refined in Phase 3.\nataD\nnairatinamuH\nrof\nserusaeM\nytilauQ\n27\n\u00a9 2023 Data Nutrition Project",
    "source_file": "pdf1.txt"
  },
  {
    "chunk_id": "pdf2_0",
    "text": "Peer Review Framework for Predictive Analytics\nin Humanitarian Response",
    "source_file": "pdf2.txt"
  },
  {
    "chunk_id": "pdf2_1",
    "text": "MODEL REPORT:\nGlobal Displacement Monitoring\nUN OCHA Centre for Humanitarian Data\nJune 2023\nModel Report:\nGlobal Displacement Monitoring\n1.Background\nThis document summarizes the documentation and findings of the peer review of the global\ndisplacement monitoring model that has been developed by the UN OCHA Centre for\nHumanitarian Data. This model creates a simple flagging systemthatusesglobaldatasources\nto raise an alert when a potential shock or humanitarian consequence may have occurred that\ncould require global attention. For this review, the focus is on a model flagging displacement\ndata, where alerts are generated based on abnormal levels of displacement seen within a\ncountryorstaticthresholdsbeingmet,withdifferenttimepoints.\nThereviewhasbeenconductedbetweenFebruaryandJune2023.\n2.MainFindingsandRecommendations\nYou can find all the documentation regarding the model, its application and the reviewprocess\natthefollowinglinks:\n\u25cf The Model Card describes version 1.0 of the model and was completed in February\n2023.\n\u25cf The Model Evaluation Matrix was completed in February 2023 by an expert on\ndisplacementdata.\n\u25cf The Implementation Plan was completed in March 2023.It summarizes how the model\noutput is used to alert the UN Central Emergency Response Fund team about a\ndeterioratingsituation.\n\u25cf The Ethical Matrix aims to identify all stakeholders and potential issues regarding the\nintended use of the model. The Ethical Matrix was completed in May 2023 by Fanny\nWeicherding,DataResponsibilityOfficerattheUNOCHACentreforHumanitarianData.\nAsummaryofthemainfindingsandrecommendationsisprovidedbelow.\n2.1TechnicalReview\nModelDevelopmentandDocumentation\nThere are no major issues that would seriously impact the validity of the model. The main\nrecommendation is to ensure that dynamic displacementsituationsarecorrectlyflaggedbythe\nmodel. Also, in addition to percentile-based thresholds, additional warning thresholdsbasedon\ntheabsolutenumberofpeopledisplacedcouldbeaddedtothemodel.\nModelEvaluation\nIt should be clarified in the documentation that the alert system has been designed with a\nspecific use case in mind and may not be appropriate for other potential use cases. It is\ntherefore recommended to better articulate the situations in which the model is expected to\nhavegoodperformancecomparedtothosethatthemodelisexpectedtomiss.\n2.2EthicalReview\nInaccuracy\nInaccuracyreferstotheoutputgeneratedbythemodelnotbeingaccurate.\nIf the model is inaccurate,itdoesnotimprovethestatusquoofresourceallocation/information\navailable to the CERF.Itisthereforerecommendedthatthemodelinaccuraciesarediscussedin\ndetailwiththeCERFteam.\nInsufficientData\nInsufficientdatareferstogapsindatatotheextentthatnoreliablepredictioncanbemade.\nFor the end user (CERF) this would be a high priority risk as it would to the team relying on a\nmodelthatdoesn\u2019tprovidereliableoutputs.\nSystematicBias\nSystematic bias refers to the datasets used to train the algorithm not reflecting the full\ncomplexityofthereality.\nThis issue is of high priority as it could potentially lead to ill informed decisions by the CERF\nteam and eventually a misallocation of resources. It is therefore recommended to always\nvalidate the information from the model with local partners and use alternative tools, - where\navailable-tocomplement/validatethemodel.\nFeedback\nThe Centre invites individuals and organizations working in the humanitarian, academic,\nresearchandprivatesectortoengagewithusonthepeerreviewprocess.Pleasesendfeedback\nontheFrameworktocentrehumdata@un.org.",
    "source_file": "pdf2.txt"
  },
  {
    "chunk_id": "pdf4_0",
    "text": "THE CENTRE FOR HUMANITARIAN DATA\nThe OCHA Centre for Humanitarian Data is focused on Our priorities in 2024\nincreasing the use and impact of data in humanitarian\nresponse.\nClosing data gaps in priority humanitarian operations.\nAccording to our latest State of Open Humanitarian\nOur vision\nData report, we estimate that 73 percent of relevant,\ncomplete data is available across 25 locations with\nWe want to create a future where everyone involved in a\nhumanitarian crises.\nhumanitarian response can access the data they need,\nwhen and how they need it, to make informed and\nSupporting OCHA\u2019s anticipatory action frameworks\nresponsible decisions. through the development and monitoring of trigger\nmechanisms in over ten countries.\nOur workstreams\nActivating adoption of the OCHA Data Responsibility\nGuidelines and the IASC Operational Guidance on\nData Responsibility.\nPromoting learning resources and guidelines and\noffer training to help humanitarians be more confident\nusing data.",
    "source_file": "pdf4.txt"
  },
  {
    "chunk_id": "pdf4_1",
    "text": "DATA SERVICES\nWe manage the Humanitarian Data Exchange (HDX),\nOCHA\u2019s open platform for sharing data across crises and Our locations\norganizations. Launched in 2014, HDX will mark 10 years\nThe Centre is based in the Netherlands in The Hague\nof operations this year.\nHumanity Hub, a co-working space for organizations\nfocused on social impact. The Centre has a small\nnumber of staff in The Hague and is further supported\nby geographically-distributed teams in Bangkok, Dakar,\nGeneva, Nairobi, and New York, among other locations.",
    "source_file": "pdf4.txt"
  },
  {
    "chunk_id": "pdf4_2",
    "text": "DATA SCIENCE\nWe use data science and risk analysis to drive preemptive The Centre will help humanitarians\ndecision making for humanitarian action. We assess models to make informed and responsible\nand forecasts for use in anticipatory action and provide\ndecisions to meet people\u2019s most urgent\nguidance on climate science and technical language.\nneeds.\nUN Secretary-General Ant\u00f3nio Guterres\nContact us",
    "source_file": "pdf4.txt"
  },
  {
    "chunk_id": "pdf4_3",
    "text": "DATA RESPONSIBILITY\nWe develop guidance and processes for managing We are here to help! Let us know how we can support\nsensitive data in a safe, ethical and effective way. your data work. We offer free services to the\nWe coordinate global inter-agency forums to increase humanitarian community ranging from data cleaning\nawareness and adoption of responsible data practices in to assessing disclosure risk of sensitive data or creating\nthe humanitarian sector. custom data visualizations.\nGet involved\n\u2022 Visit HDX to find and use humanitarian data, or sign\nup for an account to make your organization\u2019s data\naccessible to partners around the world.",
    "source_file": "pdf4.txt"
  },
  {
    "chunk_id": "pdf4_4",
    "text": "LEARNING AND PRACTICE\nWe offer in-person and remote training opportunities \u2022 Help us improve our services by signing up to\nto improve the data skills and expertise of humanitarians participate in ongoing user research.\nin technical and non-technical roles.\nWebsite: centre.humdata.org | Mailing list: bit.ly/humdatamailing | X: @humdata | YouTube: bit.ly/HumDataYouTube | LinkedIn: bit.ly/humdatalinkedin | Email: centrehumdata@un.org",
    "source_file": "pdf4.txt"
  }
]